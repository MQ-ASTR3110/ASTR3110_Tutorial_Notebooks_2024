{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASTR3110 Tutorial 1: Probabilities and Statistics\n",
    "\n",
    "Tutorial 1 of the *'Data Science Techniques in Astrophysics'* course at Macquarie University.\n",
    "\n",
    "## Learning outcomes from this tutorial\n",
    "\n",
    " * Understand what is meant by discrete and continuous probability\n",
    " * Learn how to manipulate probabilities of events\n",
    " * Know the moments used to characterise probability distributions\n",
    " * Understand the properties of the most common distributions\n",
    " * Apply knowledge to a toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Many of the most profound discoveries in modern observational astrophysics come from large surveys, often taking a data-fusion approach. When dealing with these large datasets, probabilities and statistics are required to:\n",
    "\n",
    " * Detect signals in noisy data\n",
    " * Find and investigate correlations and structure in data\n",
    " * Test a theory or hypothesis\n",
    " * Fit models to data and estimate parameters\n",
    " * Distinguish between competing models\n",
    " \n",
    "Unlike many other data-intensive fields, astronomical data is fixed - we cannot re-run our experiments (e.g., forming stars, merging galaxies, supernova) and often only have single observations to rely on. Instead we have to assume that observations of different objects constitute a reasonable sample, and we can draw reasonable conclusions from a statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability\n",
    "\n",
    "There are two philosophies that explain probability:\n",
    "\n",
    " * **Frequentist:** Probabilities are measurable frequencies assigned to specific events, i.e., events are likely to re-occur on a certain cadence.\n",
    " \n",
    " * **Bayesian:** Probability is a 'degree of belief' in the outcome of an event, allocated by an observer based on the available evidence (including prior belief).\n",
    " \n",
    "In this tutorial we will be covering the *frequentist* philosophy and moving on to the (more powerful) Bayesian methods later in the course.\n",
    "\n",
    "### Maths and notation of probabilities\n",
    "\n",
    "A *discrete* probability can be assigned to an individual event $i$ so that $P_i$ is the probability of the $i^{th}$ event occuring. A *continuous* probability $P(x)$ is the probability that random variable $x$ occurs in a process. Probabilities obey mathematical rules:\n",
    "\n",
    " 1. **Range of Probabilities:** The probability of an event or variable occuring is a real number between zero and one.\n",
    " \n",
    " $0 \\leq P(x) \\leq 1$\n",
    " \n",
    " \n",
    " 2. **Sum Rule:** Discrete probabilities sum to one.\n",
    " \n",
    " $\\sum_i P_i = 1$.\n",
    " \n",
    " For a random variable $x$ that can take any value on a continuous range, the sum becomes an integral\n",
    " \n",
    " $\\int_{-\\infty}^{+\\infty} dx ~ p(x) = 1$,\n",
    " \n",
    " where $p(x)$ is the probability density function (units of $1/x$).\n",
    " \n",
    " \n",
    " 3. **Addition Rule**: For *mutually exclusive* events $x_1, x_2, \\ldots x_n$, the probability that $x_1$ **or** $x_2$ occurs is \n",
    " \n",
    " $P(x_1 + x_2) = P(x_1) + P(x_2)$.\n",
    " \n",
    " \n",
    " 4. **Multiplication Rule**: The probability of two events $x$ **and** $y$ occuring is\n",
    " \n",
    " $P(x,y) = P(x|y) P(y) = P(y|x) P(x)$.\n",
    " \n",
    "Here $P(x|y)$ is the *conditional* probability, which is the probability of $x$ given that $y$ has occured. If the events are *mutually exclusive* this simplifies to \n",
    " \n",
    " $P(x,y) = P(x) P(y)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise in event-based probabilities\n",
    "\n",
    "This excercise is based on [notes](https://github.com/norvig/pytudes/blob/master/ipynb/Probability.ipynb) by [Peter Norvig](http://norvig.com/), the Director of Research at Google. Check out his personal pages and GitHub repository for a host of other tutorials on interesting topics. A free online textbook on probability theory is available [here](http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf) in PDF form.\n",
    "\n",
    "If you have not already done so, now is the time to setup Google colab and link it to your Google Drive account.\n",
    "\n",
    "### 1. Rolling the dice\n",
    "\n",
    "Rolling 6-sided dice illustrates some simple terms in probability theory:\n",
    "\n",
    " * **Experiment:** the act of rolling the die (or dice).\n",
    " * **Oucome:** the result of the experiment (which number faces up).\n",
    " * **Sample Space:** the set of all possible outcomes ```{1, 2, 3, 4, 5, 6}```.\n",
    " * **Event:** a subset of outcomes we are interested in e.g., ```{2, 4, 6}```.\n",
    " * **Probability:** number between ```{0, 1}``` representing certainty of event occuring.\n",
    " \n",
    "Probability of an event with respect to a sample space is the number of favorable cases (outcomes from the sample space that are in the event) divided by the total number of cases in the sample space. (This assumes that all outcomes in the sample space are equally likely.) For example the probability of rolling an **even number** on a 6-sided die is 3/6 = 1/2.\n",
    "\n",
    "We can define a probability function in code that operates on [python sets](https://docs.python.org/2/library/stdtypes.html#set-types-set-frozenset). These are unordered collections of unique objects, similar to lists, except that they don't support indexing, slicing etc. Sets can be defined using curly braces e.g., ```mySet = {1, 4, 7, 5}```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using the built-in ```Fraction``` function to return results as proper fractions. The ampersand & operator calculates the intersection $\\cap$ between sets and the pipe $|$ operator the union $\\cup$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Picking from Urns\n",
    "\n",
    "Another classical problem is predicting the likelihood of picking coloured balls from urns. \n",
    "\n",
    "*An urn contains 23 balls: 8 white, 6 blue, and 9 red. We select six balls at random (each possible selection is equally likely). What is the probability of each of these possible outcomes:*\n",
    "\n",
    " 1. all balls are red\n",
    " 2. 3 are blue, 2 are white, and 1 is red\n",
    " 3. exactly 4 balls are white\n",
    " \n",
    "So, an event here is a set of 6 balls and the sample space is the set of *all possible 6-ball combinations*. Note that we have multiple balls of the same colour and we do not care about the order of the balls (although, do care about order when counting to assess the probabilities).\n",
    "\n",
    "When counting, we will label each ball ```W1, W2,  ... W8```. For example, if you wanted to choose 3 white balls from the 8, and *cared* about the order then there are 8 ways to choose the first ball and 7 ways to choose the next and 6 the next, i.e., $8 \\times 7 \\times 6 = 336$ *permutations*. However, if we don't care about the order there are $(8 \\times 7 \\times 6) / (3 \\times 2 \\times 1) = 56$ *combinations* - we divide by $3!$ (3-[factorial](https://en.wikipedia.org/wiki/Factorial)).\n",
    "\n",
    "Let's process this problem using python. First define a function to define a set containing the contents of the urn. Here each coloured ball is labelled with a unique code like ```B3, W2```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASIDE - Note on comprehensions:** Comprehensions are python shorcuts to make simple loops (see [here](https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Comprehensions.html)) For example, converting a list of number characters to floating point numbers could be done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> or it could be done with a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **END ASIDE** \n",
    "\n",
    "Now use the function to create a set of uniquely labelled balls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the sample space, U6, as the set of all 6-ball combinations. We use *itertools.combinations* to generate the combinations and then join each combination into a space-delimited string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is 100,947 really the right number of ways of choosing 6 out of 23 items, or  \"23 choose 6\", as  mathematicians [call it](https://en.wikipedia.org/wiki/Combination)?  Well, we can choose any of 23 for the first item, any of 22 for the second, and so on down to 18 for the sixth. But we don't care about the ordering of the six items, so we divide the product by 6! (the number of permutations of 6 things) giving us:\n",
    "\n",
    "$$23 ~\\mbox{choose}~ 6 = {23 \\choose 6} = \\frac{23 \\cdot 22 \\cdot 21 \\cdot 20 \\cdot 19 \\cdot 18}{6!} = 100947$$\n",
    "\n",
    "Note that $23 \\cdot 22 \\cdot 21 \\cdot 20 \\cdot 19 \\cdot 18 = 23! \\;/\\; 17!$, so, generalizing, we can write:\n",
    "\n",
    "$$n ~\\mbox{choose}~ c = {n \\choose c} = \\frac{n!}{(n - c)! \\cdot c!}$$\n",
    "\n",
    "This is the mathematical shorthand that tells us the number of ways of choosing 6 out of 23 items.\n",
    "\n",
    "We can translate that to code and verify that 23 choose 6 is 100,947:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are redy to answer the 3 problems:\n",
    "\n",
    "**1. Probability of choosing 6 red balls**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the total number of samples is 84 because there are 9 red balls and we are asked to choose 6:\n",
    "\n",
    "$${n \\choose c} = \\frac{n!}{(n - c)! \\cdot c!} = \\frac{9!}{(9 - 6)! \\cdot 6!} = 84 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the probabilty of 6 red balls is then just 9 choose 6 divided by the size of the sample space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Probability of choosing 3 blue, 2 white and 1 red balls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the probability by multiplying the individual choice functions and dividing by the total number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choose operation already applies the factorial to account for degerate permutaations, but we could aslo expand this out further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Probability of choosing 4 white balls in any choice of 6** - \n",
    "\n",
    "We can interpret this as choosing 4 out of the 8 white balls, and 2 out of the 15 non-white balls. As before we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Probability Distributions\n",
    "\n",
    "In the exercise above we assumed that every outcome in a sample space was equally likely. When dealing with scientific data this is rarely the case. Instead, some outcomes are much more probable than others and a *probability distribution* maps a range of outcomes to frequency of those outcomes in the sample space. On other words, probability distributions characterise the spread of expected values for an event. In frequentist terms, this is the shape of the histogram of values after a large number of events have occured. As scientists, we often want to describe distributions in easy-to-understand numbers and we do this via their their *moments*.\n",
    "\n",
    "### Moments\n",
    "\n",
    "The $n^{th}$ moment of a distribution is given by \n",
    "\n",
    "$m_n = \\langle x^n \\rangle = \\int_{-\\infty}^{+\\infty} dx ~ x^n ~ p(x)$\n",
    "\n",
    "where angle brackets denote *expectation value*. \n",
    "\n",
    "* **The Zeroth Moment** gives the area of the curve under the distribution.\n",
    " \n",
    "$m_0 = \\int_{-\\infty}^{+\\infty} dx ~ p(x) = 1$.\n",
    " \n",
    "Remember that (from the sum rule) probability distributions are normalised so the integral under the curve sums to one.\n",
    "\n",
    " * **The First Moment** gives the expectation value of x, which is the **mean** of the distribution.\n",
    " \n",
    " $m_1 = \\langle x \\rangle$.\n",
    " \n",
    "Higher moments characterise the spread or shape of the distribution and it useful to calculate these as *centred moments* by shifting the distribution to have a mean of zero. The $n^{th}$ centred moment $\\mu_n$ is given by\n",
    "\n",
    "$\\mu_n = \\langle ~ (x - \\langle x \\rangle)^n ~ \\rangle$.\n",
    "\n",
    "* The **Second Centred Moment** gives the *spread* or **variance** about the mean, also denoted $\\sigma^2$:\n",
    "\n",
    "$\\mu_2 = \\sigma^2 = \\langle ~ (x - \\langle x \\rangle)^2 ~ \\rangle$.\n",
    "\n",
    "This can be multiplied out to get a simple result:\n",
    "\n",
    "$\\sigma^2 = \\langle ~ (x^2 - 2x\\langle x \\rangle + \\langle x \\rangle^2 ) ~ \\rangle = \\langle x^2 \\rangle - \\langle x \\rangle^2$, \n",
    "\n",
    "showing that in practice the variance can be calculated from the mean of the square, minus the square of the mean. The **standard deviation** $\\sigma$ is just the square-root of the variance, also known as the *root-mean-square (rms) error*.\n",
    "\n",
    "$\\sigma = rms = \\sqrt{\\langle x^2 \\rangle - \\langle x \\rangle^2}$\n",
    "\n",
    "Higher moments characterise the shape of the distribution:\n",
    "\n",
    "* The **Third Centred Moment** gives the **skewness** or asymmetry of the distribution.\n",
    "\n",
    "* The **Forth Centred Moment** give the **kurtosis** or flatness of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Uniform Distribution\n",
    "\n",
    "In the *Urn Problem* above all outcomes had the same probability, that is they were drawn from a *uniform distribution* between two limits $a$ and $b$. Probability distributions are normalised so that the area under their curves is equal to one, i.e., the probability of an outcome in the range is certain. The uniform distribution is a *top hat function* with height $1 \\big{/}(b-a)$. We can use *scipy* and the *seaborn* plotting package to sample data from a uniform distribution and visualise it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that scipy returns a Numpy array containing values between 15 and 35."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASIDE - Documentation:** You can look up the documentation for any function or module by putting a '?' at the end and executing the cell. For example, execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Two question marks will show you the python source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the distribution using the *seaborn* plotting module, based on *matplotlib*. If you get an error about it not being found, simply install it by executing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Distribution\n",
    "\n",
    "The Gaussian (or Normal) distribution is one of the most important in science as most measurements, variables values, or probabilities in a dataset or problem will be distributed in the shape of this curve. The *Central Limit Theorem* states that if you have a collection of random variables and sum them up, then the larger the collection, the closer the sum will be to a normal distribution.\n",
    "\n",
    "The equation for a normalised Gaussian distribution is\n",
    "\n",
    "$$f(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} exp \\left [ {-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\right]$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and standard deviation of the distribution, respectively, that define the shape of the curve. We can sample some data from the distribution and visualise it as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and again we can estimate the key parameters from the sampled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In science, we quote experimental results as $\\sigma$ confidence levels. For example, the if the distribution above described the background *noise* of an astronomical observation, then a result of '17' would sit at the $2\\sigma$ confidence level. That is, 95.4% of all random events (or pixels values in an image) would return values closer to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Bonus material that may not be covered during the lectorial:</font> Estimating Parameters from Skewed Distributions\n",
    "\n",
    "Distributions of real data, or even simulations, are often skewed from the ideal. However, it is still useful to estimate and describe them in terms of $\\sigma$-limits based on confidence levels. \n",
    "\n",
    "First let's generate some data by combining a few Gaussian functions into a skewed distribution. This could be the output of a measurement, the values of pixels in an image or the result of a calculation. Our job is to quote a value and uncertainty given the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data is noisy and is skewed to positive values with a long tail. We can calculate the values for mean and variance as if it were a Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the mean and $\\pm1\\sigma$ values these do not correspond the 68.3% confidence limits expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Normal distribution we would expect $(100 - 68.3)~/~2 = 15.85\\%$ of the data to lie beyond the $1\\sigma$ confidence limits. This means we must use a different method to estimate where our limits lie and adopt different upper and lower limits. The natural way to do this is to operate on the [empirical distribution function](https://en.wikipedia.org/wiki/Empirical_distribution_function) (EDF), which plots the fraction of the data under a value, as a function of all values in the data range. \n",
    "\n",
    "On the EDF plot, $x$ values which interset the $y = 0.5$, $y = 0.1585$ and $y = 0.8415$ lines correspond to the mean, $-1\\sigma$ and $+1\\sigma$ confidence limits, respectively. Let's plot the EDF of our skewed distribution versus the cumulative distribution function for a Normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than reading off the plot, what is the best way to calculate the x-values of these limits? \n",
    "\n",
    "Thankfully, Numpy provides some convenience functions to help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quoted uncertinties are the differences between the mean and the $\\pm 1\\sigma$ limits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
