{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASTR3110 Tutorial 6: Clustering with Gaussian Mixture Models\n",
    "\n",
    "Tutorial 7 of the *'Data Science Techniques in Astrophysics'* course at Macquarie University.\n",
    "\n",
    "## Learning outcomes from this tutorial\n",
    "\n",
    " * Understand the basic Gaussian Mixture modelling technique.\n",
    " * Use the Scikit Learn clustering algorithm to find clusters in 2D data.\n",
    " * Test the behaviour GMM clustering with different assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This week, we won't need to access any data on disk, so simply start a new *Python 3* notebook on Google Colab.The tutorial content is based on a section of the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas. Additional content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to GMMs\n",
    "\n",
    "In this lectorial we will take a look at Gaussian mixture models (GMMs), which can be viewed as an extension of the ideas behind *k*-means, but can also be a powerful tool for estimation beyond simple clustering.\n",
    "\n",
    "As we saw in Week 6, given simple, well-separated data, *k*-means finds suitable clustering results.\n",
    "One way to think about the k-means model is that it places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster. This radius acts as a hard cutoff for cluster assignment within the training set: any point outside this circle is not considered a member of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and set plots to appear inline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data with K Means Labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to think about the k-means model is that it places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster. This radius acts as a hard cutoff for cluster assignment within the training set: any point outside this circle is not considered a member of the cluster. We can visualize this cluster model with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def plot_kmeans(myKM, data, n_clusters=4, rseed=0, ax=None):\n",
    "    clusters = myKM.fit_predict(data)\n",
    "\n",
    "    # Plot the input data\n",
    "    ax = ax or plt.gca()#gca= get current axis (in case we are overplotting onto an exiting axis)\n",
    "    ax.axis('equal')\n",
    "    ax.scatter(data[:, 0], data[:, 1], c=clusters, s=40, cmap='jet', zorder=2)\n",
    "\n",
    "    # Plot the representation of the KMeans model\n",
    "    centers = myKM.cluster_centers_\n",
    "    #Use cdist to determine the maximum radial extent of the data points allocated to each cluster.\n",
    "    #cdist takes the x,y data points and measures distance from center\n",
    "    radii = [cdist(data[clusters == i], [center]).max() \n",
    "             for i, center in enumerate(centers)]\n",
    "    #recall zip takes two vectors and pairs them by row number as [(centers[0],radii[0]), (centers[1],radii[1])...]\n",
    "    for c, r in zip(centers, radii):\n",
    "        #add_patch lets you add different shapes onto an image. Can add circles, ellipses, triangles etc.\n",
    "        #fc=face colour, ex=edge colour, zorder tells order to plot. \n",
    "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', ec='k', lw=3, alpha=0.5, zorder=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important observation for *k*-means is that these cluster models *must be circular*: *k*-means has no built-in way of accounting for oblong or elliptical clusters.\n",
    "So, for example, if we take the same data and transform it, the cluster assignments end up becoming muddled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stretch the Y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing Expectation–Maximisation: Gaussian Mixture Models\n",
    "\n",
    "A Gaussian mixture model (GMM) attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset.\n",
    "In the simplest case, GMMs can be used for finding clusters in the same manner as *k*-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But because GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments—in Scikit-Learn this is done using the ``predict_proba`` method.\n",
    "This returns a matrix of size ``[n_samples, n_clusters]`` which measures the probability that any point belongs to the given cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, a Gaussian mixture model is very similar to *k*-means: it uses an expectation–maximization approach which qualitatively does the following:\n",
    "\n",
    "1. Choose starting guesses for the location and shape of each Gaussian component (AKA cluster)\n",
    "\n",
    "2. Repeat until converged:\n",
    "\n",
    "   1. *E-step*: for each point, find weights encoding the probability of membership in each cluster\n",
    "   2. *M-step*: for each cluster, update its location, normalization, and shape based on *all* data points, making use of the weights\n",
    "\n",
    "The result of this is that each cluster is associated not with a hard-edged sphere, but with a smooth Gaussian model.\n",
    "Just as in the *k*-means expectation–maximization approach, this algorithm can sometimes miss the globally optimal solution, and thus in practice multiple random initializations are used.\n",
    "\n",
    "Let's create a function that will help us visualize the locations and shapes of the GMM clusters by drawing ellipses based on the GMM output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is similar to the above function that adds a circle to a plot, but is now\n",
    "#generalised to an ellipse, which can have a set ellipticity and position angle\n",
    "#that comes from the covariance.\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(position, covariance, covariance_type, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    \n",
    "    \n",
    "    ax = ax or plt.gca()#gca= get current axis (in case we are overplotting onto an exiting axis)\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance_type == 'full':#shape == (2, 2):\n",
    "        #the below three lines of code convert the covariance matrix to a position angle, \n",
    "        #and two lengths for the minor and major axis of the ellipse\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    elif covariance_type == 'diag':\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    elif covariance_type == 'spherical':\n",
    "        angle = 0\n",
    "        width = 2 * np.sqrt(covariance)\n",
    "        height = 2 * np.sqrt(covariance)\n",
    "    # Draw ellipses at one-sigma, two-sigma, three-sigma and four-sigma.\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that will take in gmm and data and make a scatterplot of data with ellipses overlaid\n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        print(pos.shape, covar.shape)\n",
    "        draw_ellipse(pos, covar, gmm.covariance_type, alpha=w * w_factor, ec='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can use the GMM approach to fit our stretched dataset; allowing for a full covariance the model will fit even very oblong, stretched-out clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the covariance type\n",
    "\n",
    "If you look at the details for Gaussian Mixture, you will see that the ``covariance_type`` option can be set to different types.\n",
    "This hyperparameter controls the degrees of freedom in the shape of each cluster; it is essential to set this carefully for any given problem.\n",
    "Setting the ``covariance_type=\"diag\"`` means that the size of the cluster along each dimension can be set independently, with the resulting ellipse constrained to align with the axes.\n",
    "A slightly simpler and faster model is ``covariance_type=\"spherical\"``, which constrains the shape of the cluster such that all dimensions are equal. The resulting clustering will have similar characteristics to that of *k*-means, though it is not entirely equivalent.\n",
    "A more complicated and computationally expensive model (especially as the number of dimensions grows) is to use ``covariance_type=\"full\"``, which allows each cluster to be modeled as an ellipse with arbitrary orientation.\n",
    "\n",
    "We can see how this works using our two example datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM as *Density Estimation*\n",
    "\n",
    "Though GMM is often categorized as a clustering algorithm, fundamentally it is an algorithm for *density estimation*.\n",
    "That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic model describing the distribution of the data.\n",
    "\n",
    "As an example, consider some data generated from Scikit-Learn's ``make_moons`` function, which generates crescent-moon shaped distribtutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try 2 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try 16 components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the mixture of 16 Gaussians serves not to find separated clusters of data, but rather to model the overall *distribution* of the input data.\n",
    "This is a generative model of the distribution, meaning that the GMM gives us the recipe to generate new random data distributed similarly to our input.\n",
    "For example, here are 400 new points drawn from this 16-component GMM fit to our original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many components?\n",
    "\n",
    "In the Week 3 lectorial, we met the problem of trying to ascertain the optimum number of polynomial parameters that are required to fit the data. Optimal here meant that we want the minimum polynomial order that describes the data without over-fitting (i.e., fitting fluctuations caused by uncertainties in the data). \n",
    "\n",
    "We have a similar problem in attempting to understand how many clusters/components are required to oprtimally fit the data. Again, we can use the the [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion), which includes the likelihood measured for the fit along with a term that penalises an increase in the number of components to avoid over-fitting. Scikit-Learn's ``GMM`` estimator actually includes built-in methods that compute BIC (and another information criteria called the Akike Information Criteria), and so it is very easy to operate on this approach.\n",
    "\n",
    "Let's look at the BIC as a function as the number of GMM components for our moon dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
