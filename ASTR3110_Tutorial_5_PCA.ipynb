{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASTR3110 Tutorial 5: Principal Component Analysis\n",
    "\n",
    "Tutorial 5 of the *'Data Science Techniques in Astrophysics'* course at Macquarie University.\n",
    "\n",
    "## Learning outcomes from this tutorial\n",
    "\n",
    " * Understand what is meant by dimensionality reduction.\n",
    " * Understand what principal axes are and how they relate to variance.\n",
    " * Use SciKit Learn to identify principal axes on a simple 2D dataset.\n",
    " * Understand how to select the axes containing most information.\n",
    " * Use PCA to show how an image-based dataset decomposes into clusters in 2D.\n",
    " * Use PCA to filter noise from images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This week, we won't need to access any data on disk, so simply start a new *Python 3* notebook on Google Colab. The tutorial content is based on a section of the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas (you can access this very handy book if you are on the MQ VPN, and by signing in with your MQ email). Additional content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and set plots to appear inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is a method of reducing the *dimensionality* of multi-dimensional data. We start by identifying the axes along which information is maximised (called the *principal axes*) and can then choose to collapse one or more of the smaller (i.e., least important) of these axes by projecting the data along this axis. We can illustrate this on an artificially generated 2D dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the data on a scatter plot to get a feel for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from this plot that the most significant principal axis is along the line of best correlation and the next most significant axes is at 90-degrees to that. We can determine the vectors of these axes by using the PCA implementation in *SciKit Learn*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the MCMC fitter we encountered last week, the results are stored inside the ```myPCA``` object. For a detailed explanation access the help by executing ```myPCA??```. The important variables are ```myPCA.mean_```, ```myPCA.components_``` and ```myPCA.explained_variance_```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the vectors over the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a short function to plot an arrow\n",
    "# I'll simply explain this code to avoid typing it out.\n",
    "# Basically, it's a way to overlay arrows on a data plot.\n",
    "def draw_vector(startCoord, endCoord, ax=None):\n",
    "    \"\"\"Function to draw a vector on a plot.\"\"\"\n",
    "\n",
    "    # By default, set the axis to the current one\n",
    "    if ax is None:\n",
    "        ax = plt.gca()#gca = get current axis. This sets the axis to the axis set by plt.scatter below.\n",
    "\n",
    "    # Set the arrow properties as a dictionary\n",
    "    arrowprops = dict(arrowstyle='->', linewidth=2, color='r')\n",
    "\n",
    "    # Annotate the plot with a blank label and an arrow\n",
    "    ax.annotate('', endCoord, startCoord, arrowprops=arrowprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your data and axis directions may be different to the above plot. SciKit Learn has a function to transform the data onto the orthogonal principal axes. Do the transformation and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data onto the PCA axes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation from data axes to principal axes is an *affine transformation*, which basically means it is composed of a translation, rotation, and uniform scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA as dimensionality reduction\n",
    "\n",
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
    "\n",
    "Try using PCA for dimensionality reduction by collapsing the least-useful axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new PCA object with only one dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform this data back to understand what the effect of reducing it to a single dimension has been:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data back\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the information along the least-important axis has been removed. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
    "\n",
    "This reduced-dimension dataset is \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Visualisation\n",
    "\n",
    "Here we use a simple set of images as a toy example: small images of hand-written digits from the MNIST dataset (a very commonly used image dataset in the machine-learning world and the simplest 'hello world' example for many ML algorithms; MNIST = Modified National Institute of Standards and Technology database). \n",
    "\n",
    "SciKit Learn has access to this dataset built-in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1797 elements in the dataset, each of length 64.These are 8x8 pixel images that have been flattened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a single member of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the 1D array back to a 2D (8x8) image and plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of 8Ã—8 pixel images, meaning that they are 64-dimensional. To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project from 64 to 2 dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the first two principal components of each point to learn about the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can colorise the data using the ground-truth labels\n",
    "# that are also encoded in digits.target\n",
    "\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target,        # <- colour by ground truth label\n",
    "            edgecolor='none', \n",
    "            alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))#jet is quantised to have 10 colours. One for each of our labels 0-9\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('Projected Component 1')\n",
    "plt.ylabel('Projected Component 2')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a single tartget digit, we can see that in PCA-space they \n",
    "#often occupy relatively distinct regions.\n",
    "target=2\n",
    "plt.scatter(projected[digits.target == 2, 0], projected[digits.target == 2, 1],\n",
    "            edgecolor='none', \n",
    "            alpha=0.5,\n",
    "            fc='k')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that images containing the same hand-written digits cluster in different regions of the projected 2D space. The full data for each digit is a 64-dimensional point cloud, and the points in the plot are the projection of each data point along the 2 directions with the largest variance. Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits in two dimensions, and have done this in an unsupervised manner.\n",
    "\n",
    "You can imagine this being useful in a real world situation where you don't have the \"labels\". You can use PCA to to identify types of objects that lie in semi-distinct parts of a multi-dimensional PCA space (where the number of PCA dimensions is reduced compared with the entire original dataset). This is very useful for object classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the components mean?\n",
    "\n",
    "The PCA components can be thought of as *basis functions* - building blocks, the cumulative combination of which make up the whole picture. PCA gives the ability to order these in importance and select only the ones containing information on the *signal* in the data. See [here](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb) for a more full explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the number of components\n",
    "\n",
    "A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data.\n",
    "This can be determined by looking at the cumulative *explained variance ratio* as a function of the number of components. This is the ratio of the variance explained by the currently accepted components to the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the PCA components and do not restrict the\n",
    "# number of components found (fully describe the data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This curve quantifies how much of the total, 64-dimensional variance is contained within the first $N$ components.\n",
    "For example, we see that with the digits the first 10 components contain approximately 75% of the variance, while you need around 50 components to describe close to 100% of the variance.\n",
    "\n",
    "A good selection here might be choosing to fit 20 components to retain 90% of the information. Note that that last 10% may be dominated by noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA as Noise Filtering\n",
    "\n",
    "PCA can also be used as a filtering approach for noisy data.\n",
    "The idea is this: any components with variance much larger than the effect of the noise should be relatively unaffected by the noise.\n",
    "So if you reconstruct the data using just the largest subset of principal components, you should be preferentially keeping the signal and throwing out the noise.\n",
    "\n",
    "Let's see how this looks with the digits data. First we will plot several of the input noise-free data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 40 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a function to plot the images\n",
    "def plot_digits(data, nRows=4):\n",
    "    \"\"\"Plot rows of images from the MNIST digits dataset.\"\"\"\n",
    "    \n",
    "    \n",
    "    # Setup the figure\n",
    "    fig = plt.figure(figsize=(10, nRows))\n",
    "    axes = fig.subplots(nRows, 10, \n",
    "                        subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                        gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    \n",
    "    # Loop through the axes, plotting the images\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8, 8),\n",
    "                  cmap='gray_r', interpolation='nearest',\n",
    "                  clim=(0, 16))\n",
    "\n",
    "# Call the plotting function to plot the 1st 4 rows\n",
    "plot_digits(digits.data, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add some noise to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add some random noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A human observer can still make out which digits are in the images. Now see how PCA can identify the axes with information content and then filter on these to remove noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 50% of the variance amounts to 12 principal components.\n",
    "Now we compute these components, and then use the inverse of the transform to reconstruct the filtered digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data into PCA space with 12 axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare with the original and noisy data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
